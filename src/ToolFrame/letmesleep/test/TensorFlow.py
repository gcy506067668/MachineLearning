import osimport tensorflow as tfimport numpy as npimport mathimport timeitimport matplotlib.pyplot as plt%matplotlib inlinedef load_cifar10(num_training=49000, num_validation=1000, num_test=10000):    """    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare    it for the two-layer neural net classifier. These are the same steps as    we used for the SVM, but condensed to a single function.    """    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes    cifar10 = tf.keras.datasets.cifar10.load_data()    (X_train, y_train), (X_test, y_test) = cifar10    X_train = np.asarray(X_train, dtype=np.float32)    y_train = np.asarray(y_train, dtype=np.int32).flatten()    X_test = np.asarray(X_test, dtype=np.float32)    y_test = np.asarray(y_test, dtype=np.int32).flatten()    # Subsample the data    mask = range(num_training, num_training + num_validation)    X_val = X_train[mask]    y_val = y_train[mask]    mask = range(num_training)    X_train = X_train[mask]    y_train = y_train[mask]    mask = range(num_test)    X_test = X_test[mask]    y_test = y_test[mask]    # Normalize the data: subtract the mean pixel and divide by std    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)    X_train = (X_train - mean_pixel) / std_pixel    X_val = (X_val - mean_pixel) / std_pixel    X_test = (X_test - mean_pixel) / std_pixel    return X_train, y_train, X_val, y_val, X_test, y_test# If there are errors with SSL downloading involving self-signed certificates,# it may be that your Python version was recently installed on the current machine.# See: https://github.com/tensorflow/tensorflow/issues/10779# To fix, run the command: /Applications/Python\ 3.7/Install\ Certificates.command#   ...replacing paths as necessary.# Invoke the above function to get our data.NHW = (0, 1, 2)X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()print('Train data shape: ', X_train.shape)print('Train labels shape: ', y_train.shape, y_train.dtype)print('Validation data shape: ', X_val.shape)print('Validation labels shape: ', y_val.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape)class Dataset(object):    def __init__(self, X, y, batch_size, shuffle=False):        """        Construct a Dataset object to iterate over data X and labels y                Inputs:        - X: Numpy array of data, of any shape        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]        - batch_size: Integer giving number of elements per minibatch        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch        """        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'        self.X, self.y = X, y        self.batch_size, self.shuffle = batch_size, shuffle    def __iter__(self):        N, B = self.X.shape[0], self.batch_size        idxs = np.arange(N)        if self.shuffle:            np.random.shuffle(idxs)        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)test_dset = Dataset(X_test, y_test, batch_size=64)# We can iterate through a dataset like this:for t, (x, y) in enumerate(train_dset):    print(t, x.shape, y.shape)    if t > 5: break# Set up some global variablesUSE_GPU = Trueif USE_GPU:    device = '/device:GPU:0'else:    device = '/cpu:0'# Constant to control how often we print when training modelsprint_every = 100print('Using device: ', device)def flatten(x):    """        Input:    - TensorFlow Tensor of shape (N, D1, ..., DM)        Output:    - TensorFlow Tensor of shape (N, D1 * ... * DM)    """    N = tf.shape(x)[0]    return tf.reshape(x, (N, -1))def test_flatten():    # Construct concrete values of the input data x using numpy    x_np = np.arange(24).reshape((2, 3, 4))    print('x_np:\n', x_np, '\n')    # Compute a concrete output value.    x_flat_np = flatten(x_np)    print('x_flat_np:\n', x_flat_np, '\n')test_flatten()def two_layer_fc(x, params):    """    A fully-connected neural network; the architecture is:    fully-connected layer -> ReLU -> fully connected layer.    Note that we only need to define the forward pass here; TensorFlow will take    care of computing the gradients for us.        The input to the network will be a minibatch of data, of shape    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,    and the output layer will produce scores for C classes.    Inputs:    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of      input data.    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the      network, where w1 has shape (D, H) and w2 has shape (H, C).        Returns:    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores      for the input data x.    """    w1, w2 = params                   # Unpack the parameters    x = flatten(x)                    # Flatten the input; now x has shape (N, D)    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)    return scoresdef two_layer_fc_test():    hidden_layer_size = 42    # Scoping our TF operations under a tf.device context manager     # lets us tell TensorFlow where we want these Tensors to be    # multiplied and/or operated on, e.g. on a CPU or a GPU.    with tf.device(device):                x = tf.zeros((64, 32, 32, 3))        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))        w2 = tf.zeros((hidden_layer_size, 10))        # Call our two_layer_fc function for the forward pass of the network.        scores = two_layer_fc(x, [w1, w2])    print(scores.shape)two_layer_fc_test()def three_layer_convnet(x, params):    """    A three-layer convolutional network with the architecture described above.        Inputs:    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images    - params: A list of TensorFlow Tensors giving the weights and biases for the      network; should contain the following:      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving        weights for the first convolutional layer.      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the        first convolutional layer.      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)        giving weights for the second convolutional layer      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the        second convolutional layer.      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.        Can you figure out what the shape should be?      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.        Can you figure out what the shape should be?    """    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params    scores = None    ############################################################################    # TODO: Implement the forward pass for the three-layer ConvNet.            #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                              END OF YOUR CODE                            #    ############################################################################    return scoresdef three_layer_convnet_test():        with tf.device(device):        x = tf.zeros((64, 32, 32, 3))        conv_w1 = tf.zeros((5, 5, 3, 6))        conv_b1 = tf.zeros((6,))        conv_w2 = tf.zeros((3, 3, 6, 9))        conv_b2 = tf.zeros((9,))        fc_w = tf.zeros((32 * 32 * 9, 10))        fc_b = tf.zeros((10,))        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]        scores = three_layer_convnet(x, params)    # Inputs to convolutional layers are 4-dimensional arrays with shape    # [batch_size, height, width, channels]    print('scores_np has shape: ', scores.shape)three_layer_convnet_test()def training_step(model_fn, x, y, params, learning_rate):    with tf.GradientTape() as tape:        scores = model_fn(x, params) # Forward pass of the model        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)        total_loss = tf.reduce_mean(loss)        grad_params = tape.gradient(total_loss, params)        # Make a vanilla gradient descent step on all of the model parameters        # Manually update the weights using assign_sub()        for w, grad_w in zip(params, grad_params):            w.assign_sub(learning_rate * grad_w)                                return total_lossdef train_part2(model_fn, init_fn, learning_rate):    """    Train a model on CIFAR-10.        Inputs:    - model_fn: A Python function that performs the forward pass of the model      using TensorFlow; it should have the following signature:      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a      minibatch of image data, params is a list of TensorFlow Tensors holding      the model weights, and scores is a TensorFlow Tensor of shape (N, C)      giving scores for all elements of x.    - init_fn: A Python function that initializes the parameters of the model.      It should have the signature params = init_fn() where params is a list      of TensorFlow Tensors holding the (randomly initialized) weights of the      model.    - learning_rate: Python float giving the learning rate to use for SGD.    """            params = init_fn()  # Initialize the model parameters                        for t, (x_np, y_np) in enumerate(train_dset):        # Run the graph on a batch of training data.        loss = training_step(model_fn, x_np, y_np, params, learning_rate)                # Periodically print the loss and check accuracy on the val set.        if t % print_every == 0:            print('Iteration %d, loss = %.4f' % (t, loss))            check_accuracy(val_dset, x_np, model_fn, params)def check_accuracy(dset, x, model_fn, params):    """    Check accuracy on a classification model, e.g. for validation.        Inputs:    - dset: A Dataset object against which to check accuracy    - x: A TensorFlow placeholder Tensor where input images should be fed    - model_fn: the Model we will be calling to make predictions on x    - params: parameters for the model_fn to work with          Returns: Nothing, but prints the accuracy of the model    """    num_correct, num_samples = 0, 0    for x_batch, y_batch in dset:        scores_np = model_fn(x_batch, params).numpy()        y_pred = scores_np.argmax(axis=1)        num_samples += x_batch.shape[0]        num_correct += (y_pred == y_batch).sum()    acc = float(num_correct) / num_samples    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))def create_matrix_with_kaiming_normal(shape):    if len(shape) == 2:        fan_in, fan_out = shape[0], shape[1]    elif len(shape) == 4:        fan_in, fan_out = np.prod(shape[:3]), shape[3]    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)def two_layer_fc_init():    """    Initialize the weights of a two-layer network, for use with the    two_layer_network function defined above.     You can use the `create_matrix_with_kaiming_normal` helper!        Inputs: None        Returns: A list of:    - w1: TensorFlow tf.Variable giving the weights for the first layer    - w2: TensorFlow tf.Variable giving the weights for the second layer    """    hidden_layer_size = 4000    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))    return [w1, w2]learning_rate = 1e-2train_part2(two_layer_fc, two_layer_fc_init, learning_rate)def three_layer_convnet_init():    """    Initialize the weights of a Three-Layer ConvNet, for use with the    three_layer_convnet function defined above.    You can use the `create_matrix_with_kaiming_normal` helper!        Inputs: None        Returns a list containing:    - conv_w1: TensorFlow tf.Variable giving weights for the first conv layer    - conv_b1: TensorFlow tf.Variable giving biases for the first conv layer    - conv_w2: TensorFlow tf.Variable giving weights for the second conv layer    - conv_b2: TensorFlow tf.Variable giving biases for the second conv layer    - fc_w: TensorFlow tf.Variable giving weights for the fully-connected layer    - fc_b: TensorFlow tf.Variable giving biases for the fully-connected layer    """    params = None    ############################################################################    # TODO: Initialize the parameters of the three-layer network.              #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                             END OF YOUR CODE                             #    ############################################################################    return paramslearning_rate = 3e-3train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)class TwoLayerFC(tf.keras.Model):    def __init__(self, hidden_size, num_classes):        super(TwoLayerFC, self).__init__()                initializer = tf.initializers.VarianceScaling(scale=2.0)        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',                                   kernel_initializer=initializer)        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',                                   kernel_initializer=initializer)        self.flatten = tf.keras.layers.Flatten()        def call(self, x, training=False):        x = self.flatten(x)        x = self.fc1(x)        x = self.fc2(x)        return xdef test_TwoLayerFC():    """ A small unit test to exercise the TwoLayerFC model above. """    input_size, hidden_size, num_classes = 50, 42, 10    x = tf.zeros((64, input_size))    model = TwoLayerFC(hidden_size, num_classes)    with tf.device(device):        scores = model(x)        print(scores.shape)        test_TwoLayerFC()class ThreeLayerConvNet(tf.keras.Model):    def __init__(self, channel_1, channel_2, num_classes):        super(ThreeLayerConvNet, self).__init__()        ########################################################################        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #        # should instantiate layer objects to be used in the forward pass.     #        ########################################################################        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        pass        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        ########################################################################        #                           END OF YOUR CODE                           #        ########################################################################            def call(self, x, training=False):        scores = None        ########################################################################        # TODO: Implement the forward pass for a three-layer ConvNet. You      #        # should use the layer objects defined in the __init__ method.         #        ########################################################################        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        pass        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        ########################################################################        #                           END OF YOUR CODE                           #        ########################################################################                return scoresdef test_ThreeLayerConvNet():        channel_1, channel_2, num_classes = 12, 8, 10    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)    with tf.device(device):        x = tf.zeros((64, 3, 32, 32))        scores = model(x)        print(scores.shape)test_ThreeLayerConvNet()def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):    """    Simple training loop for use with models defined using tf.keras. It trains    a model for one epoch on the CIFAR-10 training set and periodically checks    accuracy on the CIFAR-10 validation set.        Inputs:    - model_init_fn: A function that takes no parameters; when called it      constructs the model we want to train: model = model_init_fn()    - optimizer_init_fn: A function which takes no parameters; when called it      constructs the Optimizer object we will use to optimize the model:      optimizer = optimizer_init_fn()    - num_epochs: The number of epochs to train for        Returns: Nothing, but prints progress during trainingn    """        with tf.device(device):        # Compute the loss like we did in Part II        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()                model = model_init_fn()        optimizer = optimizer_init_fn()                train_loss = tf.keras.metrics.Mean(name='train_loss')        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')            val_loss = tf.keras.metrics.Mean(name='val_loss')        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')                t = 0        for epoch in range(num_epochs):                        # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics            train_loss.reset_states()            train_accuracy.reset_states()                        for x_np, y_np in train_dset:                with tf.GradientTape() as tape:                                        # Use the model function to build the forward pass.                    scores = model(x_np, training=is_training)                    loss = loss_fn(y_np, scores)                          gradients = tape.gradient(loss, model.trainable_variables)                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))                                        # Update the metrics                    train_loss.update_state(loss)                    train_accuracy.update_state(y_np, scores)                                        if t % print_every == 0:                        val_loss.reset_states()                        val_accuracy.reset_states()                        for test_x, test_y in val_dset:                            # During validation at end of epoch, training set to False                            prediction = model(test_x, training=False)                            t_loss = loss_fn(test_y, prediction)                            val_loss.update_state(t_loss)                            val_accuracy.update_state(test_y, prediction)                                                template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'                        print (template.format(t, epoch+1,                                             train_loss.result(),                                             train_accuracy.result()*100,                                             val_loss.result(),                                             val_accuracy.result()*100))                    t += 1hidden_size, num_classes = 4000, 10learning_rate = 1e-2def model_init_fn():    return TwoLayerFC(hidden_size, num_classes)def optimizer_init_fn():    return tf.keras.optimizers.SGD(learning_rate=learning_rate)train_part34(model_init_fn, optimizer_init_fn)learning_rate = 3e-3channel_1, channel_2, num_classes = 32, 16, 10def model_init_fn():    model = None    ############################################################################    # TODO: Complete the implementation of model_fn.                           #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                           END OF YOUR CODE                               #    ############################################################################    return modeldef optimizer_init_fn():    optimizer = None    ############################################################################    # TODO: Complete the implementation of model_fn.                           #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                           END OF YOUR CODE                               #    ############################################################################    return optimizertrain_part34(model_init_fn, optimizer_init_fn)learning_rate = 1e-2def model_init_fn():    input_shape = (32, 32, 3)    hidden_layer_size, num_classes = 4000, 10    initializer = tf.initializers.VarianceScaling(scale=2.0)    layers = [        tf.keras.layers.Flatten(input_shape=input_shape),        tf.keras.layers.Dense(hidden_layer_size, activation='relu',                              kernel_initializer=initializer),        tf.keras.layers.Dense(num_classes, activation='softmax',                               kernel_initializer=initializer),    ]    model = tf.keras.Sequential(layers)    return modeldef optimizer_init_fn():    return tf.keras.optimizers.SGD(learning_rate=learning_rate) train_part34(model_init_fn, optimizer_init_fn)model = model_init_fn()model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),              loss='sparse_categorical_crossentropy',              metrics=[tf.keras.metrics.sparse_categorical_accuracy])model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))model.evaluate(X_test, y_test)def model_init_fn():    model = None    ############################################################################    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                            END OF YOUR CODE                              #    ############################################################################    return modellearning_rate = 5e-4def optimizer_init_fn():    optimizer = None    ############################################################################    # TODO: Complete the implementation of model_fn.                           #    ############################################################################    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    pass    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****    ############################################################################    #                           END OF YOUR CODE                               #    ############################################################################    return optimizertrain_part34(model_init_fn, optimizer_init_fn)model = model_init_fn()model.compile(optimizer='sgd',              loss='sparse_categorical_crossentropy',              metrics=[tf.keras.metrics.sparse_categorical_accuracy])model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))model.evaluate(X_test, y_test)def two_layer_fc_functional(input_shape, hidden_size, num_classes):      initializer = tf.initializers.VarianceScaling(scale=2.0)    inputs = tf.keras.Input(shape=input_shape)    flattened_inputs = tf.keras.layers.Flatten()(inputs)    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',                                 kernel_initializer=initializer)(flattened_inputs)    scores = tf.keras.layers.Dense(num_classes, activation='softmax',                             kernel_initializer=initializer)(fc1_output)    # Instantiate the model given inputs and outputs.    model = tf.keras.Model(inputs=inputs, outputs=scores)    return modeldef test_two_layer_fc_functional():    """ A small unit test to exercise the TwoLayerFC model above. """    input_size, hidden_size, num_classes = 50, 42, 10    input_shape = (50,)        x = tf.zeros((64, input_size))    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)        with tf.device(device):        scores = model(x)        print(scores.shape)        test_two_layer_fc_functional()input_shape = (32, 32, 3)hidden_size, num_classes = 4000, 10learning_rate = 1e-2def model_init_fn():    return two_layer_fc_functional(input_shape, hidden_size, num_classes)def optimizer_init_fn():    return tf.keras.optimizers.SGD(learning_rate=learning_rate)train_part34(model_init_fn, optimizer_init_fn)class CustomConvNet(tf.keras.Model):    def __init__(self):        super(CustomConvNet, self).__init__()        ############################################################################        # TODO: Construct a model that performs well on CIFAR-10                   #        ############################################################################        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        pass        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        ############################################################################        #                            END OF YOUR CODE                              #        ############################################################################        def call(self, input_tensor, training=False):        ############################################################################        # TODO: Construct a model that performs well on CIFAR-10                   #        ############################################################################        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        pass        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        ############################################################################        #                            END OF YOUR CODE                              #        ############################################################################                return xdevice = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!# device = '/cpu:0'        # Change this to a CPU/GPU as you wish!print_every = 700num_epochs = 10model = CustomConvNet()def model_init_fn():    return CustomConvNet()def optimizer_init_fn():    learning_rate = 1e-3    return tf.keras.optimizers.Adam(learning_rate) train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)